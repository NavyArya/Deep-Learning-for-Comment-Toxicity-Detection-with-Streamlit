# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpf7uIrVQxmMMSBHsVxqYXGaFZ-x9c22

# PROJECT NAME - Deep Learning for Comment Toxicity Detection with Streamlit

PROJECT SUMMARY - The project "Deep Learning for Comment Toxicity Detection with Streamlit" focuses on building an automated system to identify and flag toxic comments in online communities and social media platforms. Toxic comments include harassment, hate speech, and offensive language that harm healthy online discourse. Using deep learning techniques (LSTM, CNN, or transformer-based models like BERT), the project preprocesses and trains a model to predict the toxicity likelihood of textual comments.

The system is deployed through a real-time interactive web application built with Streamlit, allowing users to enter comments and get immediate toxicity predictions. It also supports bulk comment analysis via CSV upload for efficient moderation of large volumes of user-generated content. The project aims to assist platform moderators, content moderation services, brands, and educational websites in maintaining safe and constructive online environments.

Deliverables include the fully functional Streamlit app, trained model source code, deployment guide, and demonstration video. The solution helps automate the moderation process, enhancing brand safety, user experience, and online community health by timely and accurate detection of toxic content.

# GitHub Link -

# Problem Statement - Online communities and social media platforms have become integral parts of modern communication, facilitating interactions and discussions on various topics. However, the prevalence of toxic comments, which include harassment, hate speech, and offensive language, poses significant challenges to maintaining healthy and constructive online discourse. To address this issue, there is a pressing need for automated systems capable of detecting and flagging toxic comments in real-time.
The objective of this project is to develop a deep learning-based comment toxicity model using Python. This model will analyze text input from online comments and predict the likelihood of each comment being toxic. By accurately identifying toxic comments, the model will assist platform moderators and administrators in taking appropriate actions to mitigate the negative impact of toxic behavior, such as filtering, warning users, or initiating further review processes.

# Let's Begin !
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

train_path = '/content/drive/MyDrive/train.csv'
test_path = '/content/drive/MyDrive/test.csv'
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)

print("\nTrain columns:", train_df.columns)
print("Test columns:", test_df.columns)

print("\nFirst 5 train rows:\n", train_df.head())
print("\nFirst 5 test rows:\n", test_df.head())

print("\nMissing values in train:\n", train_df.isnull().sum())
print("\nMissing values in test:\n", test_df.isnull().sum())

label_col = 'toxic'  # update as needed
if label_col in train_df.columns:
    plt.figure(figsize=(6,4))
    sns.countplot(x=label_col, data=train_df)
    plt.title('Toxic vs Non-toxic Comment Distribution')
    plt.xlabel('Toxic (0: Non-toxic, 1: Toxic)')
    plt.ylabel('Count')
    plt.show()
    print("\nValue counts for target column:")
    print(train_df[label_col].value_counts())
else:
    print(f"\nColumn '{label_col}' not found in training data.")

comment_col = 'comment_text'  # update as needed
if comment_col in train_df.columns:
    print("\nSample train comments:")
    print(train_df[comment_col].sample(5).values)
else:
    print(f"\nColumn '{comment_col}' not found in training data.")

import nltk
import os

# Set a writable directory for NLTK data
nltk_data_path = '/content/nltk_data'
os.makedirs(nltk_data_path, exist_ok=True)
nltk.data.path.append(nltk_data_path)

print(f"NLTK data path set to: {nltk.data.path}")

# Install nltk if not already installed
!pip install nltk --quiet

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Added this line to download the missing resource


# Define text cleaning function
def clean_text(text):
    text = str(text).lower()  # Lowercase
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'[^a-z\s]', '', text)  # Remove special characters and digits
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Prepare stopwords set
stop_words = set(stopwords.words('english'))

# Function for tokenization and stopword removal
def preprocess_text(text):
    text = clean_text(text)
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

# Apply preprocessing to your text column
# Example: df['cleaned_text'] = df['comment_text'].apply(preprocess_text)

# Vectorize the cleaned text using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
# Example: tfidf_features = vectorizer.fit_transform(df['cleaned_text'])

# You can then use tfidf_features as input for your machine learning models

# -- Sample Usage with dummy data --
sample_texts = [
    "Hello, this is a sample comment! Visit https://example.com",
    "This is another comment, with some stopwords and numbers 1234."
]

# Preprocess
cleaned_samples = [preprocess_text(text) for text in sample_texts]

# Vectorize
tfidf_matrix = vectorizer.fit_transform(cleaned_samples)

print("Cleaned Samples:", cleaned_samples)
print("TF-IDF Matrix shape:", tfidf_matrix.shape)

# Install necessary libraries
!pip install tensorflow --quiet

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from google.colab import drive # Import drive

# Mount Google Drive within the cell
drive.mount('/content/drive')


# Load your dataset - update the path accordingly
train_path = '/content/drive/MyDrive/train.csv' # Define the path within the cell
train_df = pd.read_csv(train_path)

# Check dataset columns
print("Columns in dataset:", train_df.columns)
label_col = 'toxic'  # change if your label column name differs

# Basic preprocessing parameters
max_words = 10000  # vocabulary size
max_len = 100      # max number of tokens per comment

# Prepare texts and labels
texts = train_df['comment_text'].astype(str).values
labels = train_df[label_col].values

# Tokenize text
tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential([
    Embedding(max_words, 64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print model summary
model.summary()

# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=64,
    callbacks=[early_stopping]
)

# Evaluate on validation set
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import numpy as np

# Assuming you have a DataFrame 'df' with 'comment_text' and 'toxic' columns
texts = train_df['comment_text'].values
labels = train_df['toxic'].values

# Tokenize and pad sequences
max_words = 10000
max_len = 100
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

# Split data
X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Model architecture
model_lstm = Sequential([
    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),
    Bidirectional(LSTM(64)),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train
model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))

from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D

model_cnn = Sequential([
    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),
    Conv1D(128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model_cnn.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))

model.save('/content/drive/MyDrive/saved_model/my_model.h5')

# Option 2: Save as single HDF5 file
# model.save('/content/drive/MyDrive/my_model.h5')  # Uncomment to use instead

# ==== Loading the model later ====

# Load SavedModel directory
# loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_model/my_model_dir')

# Load HDF5 file (if saved that way)
loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/saved_model/my_model.h5')